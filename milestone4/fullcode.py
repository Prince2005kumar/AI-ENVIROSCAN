# -*- coding: utf-8 -*-
"""enviro.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DJnZZ3xdfFYbsxuJteRHBqZmxgKnIeoT
"""

!pip install opendatasets

import opendatasets as op
op.download("https://www.kaggle.com/datasets/omkargowda/indian-cities-lat-long/data")
#"94eee4b1fb8e4c7ecfea428ed5b3fc18"

import pandas as pd
df = pd.read_csv("/content/city_air_weather_osm.csv")
df.shape
# df.head()

import pandas as pd
import requests
from datetime import datetime

# -----------------------------
# Step 1: Load the city dataset
# -----------------------------
# Corrected file path based on directory listing
df = pd.read_csv("/content/indian-cities-lat-long/Indian_cities.csv")

# Let's pick first 5 cities for testing
test_cities = df.head(3)  # you can change the number later

# -----------------------------
# Step 2: Your OpenWeatherMap API Key
# -----------------------------
OWM_KEY = "13bd8cad6b6a28245eff4047fb57163f"  # <<--- Replace with your OpenWeatherMap API key

# -----------------------------
# Step 3: Function to call Air Pollution API
# -----------------------------
def fetch_air_pollution(lat, lon, api_key):
    url = f"http://api.openweathermap.org/data/2.5/air_pollution?lat={lat}&lon={lon}&appid={api_key}"
    resp = requests.get(url)
    if resp.status_code != 200:
        print(f"Error fetching data for {lat}, {lon}: {resp.status_code}")
        return None
    data = resp.json()
    return data

# -----------------------------
# Step 4: Test the API for a few cities
# -----------------------------
for idx, row in test_cities.iterrows():
    city = row['City']        # adjust column name if different
    lat = row['Latitude']     # adjust column name if different
    lon = row['Longitude']    # adjust column name if different

    print(f"\nFetching air pollution data for {city} ({lat}, {lon})...")
    api_data = fetch_air_pollution(lat, lon, OWM_KEY)

    if api_data:
        # Display main fields from API
        for item in api_data.get('list', []):
            dt = datetime.utcfromtimestamp(item['dt'])
            components = item['components']
            print(f"Timestamp (UTC): {dt}")
            print(f"PM2.5: {components.get('pm2_5')}, PM10: {components.get('pm10')}, NO2: {components.get('no2')}, CO: {components.get('co')}, SO2: {components.get('so2')}, O3: {components.get('o3')}")

!pip install osmnx

import pandas as pd
import requests
from datetime import datetime, UTC
import osmnx as ox
import os
import time # Import time for potential pauses

from shapely.geometry import Polygon

# -----------------------------
# Load city dataset
# -----------------------------
df = pd.read_csv("/content/indian-cities-lat-long/Indian_cities.csv")
# We will process all cities, but in batches
cities_to_process = df.copy()

# -----------------------------
# API Key
# -----------------------------
API_KEY = "13bd8cad6b6a28245eff4047fb57163f"   # <-- replace with your API key

# Define the output file name
output_csv_file = "city_air_weather_osm.csv"

# Load existing data if the file already exists to resume
all_data = []
if os.path.exists(output_csv_file):
    # print(f"Loading existing data from {output_csv_file}...")
    existing_df = pd.read_csv(output_csv_file)
    all_data = existing_df.to_dict('records')
    processed_cities = existing_df['city'].tolist()
    # print(f"Loaded {len(all_data)} records. {len(cities_to_process) - len(processed_cities)} cities remaining.")
else:
    processed_cities = []


# -----------------------------
# Fetch Air Pollution Data
# -----------------------------
def get_air_quality(lat, lon):
    url = f"http://api.openweathermap.org/data/2.5/air_pollution?lat={lat}&lon={lon}&appid={API_KEY}"
    resp = requests.get(url)
    # print(f"Air pollution API response status code for {lat}, {lon}: {resp.status_code}")
    if resp.status_code == 200:
        return resp.json()
    return None

# -----------------------------
# Fetch Climate/Weather Data
# -----------------------------
def get_weather(lat, lon):
    # Changed to current weather endpoint for testing
    url = f"http://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={API_KEY}&units=metric"
    resp = requests.get(url)
    # print(f"Weather API response status code for {lat}, {lon}: {resp.status_code}")
    if resp.status_code == 200:
        return resp.json()
    return None

# -----------------------------
# Fetch OSM Features (roads, industries, dump sites, farmland)
# -----------------------------
def get_osm_features(city_name):
    features = {"road_count": None, "industrial_count": None, "dump_count": None, "farmland_count": None}
    try:
        # Define tags for the features we want to retrieve
        tags = {"highway": True,
                "landuse": ["industrial", "farmland"],
                "amenity": "waste_disposal"}

        # Fetch features using ox.features_from_place
        osm_features = ox.features_from_place(city_name + ", India", tags) # Added ", India" for better place identification

        # Count the features by type
        # Ensure columns exist before accessing
        features["road_count"] = len(osm_features[osm_features.geom_type.isin(['LineString', 'MultiLineString'])]) if 'highway' in osm_features.columns else None
        features["industrial_count"] = len(osm_features[osm_features['landuse'] == 'industrial']) if 'landuse' in osm_features.columns else None
        features["dump_count"] = len(osm_features[osm_features['amenity'] == 'waste_disposal']) if 'amenity' in osm_features.columns else None
        features["farmland_count"] = len(osm_features[osm_features['landuse'] == 'farmland']) if 'landuse' in osm_features.columns else None


    except Exception as e:

        # print(f"OSM fetch failed for {city_name}: {e}")
        # Consider adding a small pause here to avoid overwhelming the API
        # time.sleep(1)
        pass
    return features

# -----------------------------
# Collect Data for Cities in Batches
# -----------------------------
batch_size = 20 # Define batch size
processed_count = len(processed_cities)

for index, row in cities_to_process.iterrows():
    city = row['City']

    # Skip if city has already been processed
    if city in processed_cities:
        # print(f"Skipping {city}, already processed.")
        continue

    lat = row['Latitude']
    lon = row['Longitude']

    # print(f"\nFetching data for {city} ({processed_count + 1}/{len(cities_to_process)})...")

    pollution = get_air_quality(lat, lon)
    weather   = get_weather(lat, lon)
    osm_feat  = get_osm_features(city)

    if pollution and weather:
        try:
            # take first entry for pollution
            p = pollution['list'][0]
            poll_comp = p['components']
            poll_time = datetime.fromtimestamp(p['dt'], UTC)

            # take first entry for weather
            w = weather
            main_weather = w['main']
            wind_weather = w['wind']
            clouds_weather = w['clouds']

            all_data.append({
                "city": city,
                "lat": lat,
                "lon": lon,
                "timestamp": poll_time,

                # Pollution
                "pm2_5": poll_comp.get('pm2_5'),
                "pm10": poll_comp.get('pm10'),
                "no2": poll_comp.get('no2'),
                "co": poll_comp.get('co'),
                "so2": poll_comp.get('so2'),
                "o3": poll_comp.get('o3'),

                # Weather - Adjusted for current weather data
                "temp_day": main_weather.get('temp'),
                "temp_min": main_weather.get('temp_min'),
                "temp_max": main_weather.get('temp_max'),
                "humidity": main_weather.get('humidity'),
                "pressure": main_weather.get('pressure'),
                "wind_speed": wind_weather.get('speed'),
                "wind_deg": wind_weather.get('deg'),
                "clouds": clouds_weather.get('all'),

                # OSM Features
                "road_count": osm_feat.get("road_count"),
                "industrial_count": osm_feat.get("industrial_count"),
                "dump_count": osm_feat.get("dump_count"),
                "farmland_count": osm_feat.get("farmland_count")
            })
            processed_count += 1
            processed_cities.append(city) # Add city to processed list

        except Exception as e:
            # print(f"Error processing data for {city}: {e}")
            pass # Continue to next city if there's an error with data structure


    # Save incrementally after each batch
    if processed_count % batch_size == 0 or processed_count == len(cities_to_process):
        # print(f"Saving {processed_count} records to {output_csv_file}...")
        final_df = pd.DataFrame(all_data)
        final_df.to_csv(output_csv_file, index=False)
        # print("Saved.")
        # Optional: Add a small pause between batches
        # time.sleep(5)


# print("\nData collection complete.")
# print(f"Final dataset size: {len(all_data)} records.")

# Final save (in case the last batch size was less than batch_size)
final_df = pd.DataFrame(all_data)
final_df.to_csv(output_csv_file, index=False)
print("Final dataset saved.")

import pandas as pd
import numpy as np

# Load CSV
df = pd.read_csv("/content/city_air_weather_osm.csv")

# -----------------------------
# 1. Handle Missing Values
# -----------------------------
numeric_cols = ['pm2_5', 'pm10', 'no2', 'co', 'so2', 'o3',
                'temp_day', 'temp_min', 'temp_max', 'humidity', 'pressure',
                'wind_speed', 'wind_deg', 'clouds',
                'road_count', 'industrial_count', 'dump_count', 'farmland_count']

for col in numeric_cols:
    df[col] = df[col].fillna(df[col].median())

# -----------------------------
# 2. Convert Timestamp
# -----------------------------
df['timestamp'] = pd.to_datetime(df['timestamp'])

# -----------------------------
# 3. Encode City Names
# -----------------------------
df['city_code'] = df['city'].astype('category').cat.codes

# -----------------------------
# 4. OSM Feature Engineering
# -----------------------------
# Example: convert raw counts to relative features
# You can normalize by max count across all cities
df['road_density'] = df['road_count'] / df['road_count'].max()
df['industrial_density'] = df['industrial_count'] / df['industrial_count'].max()
df['dump_density'] = df['dump_count'] / df['dump_count'].max()
df['farmland_density'] = df['farmland_count'] / df['farmland_count'].max()

# Optionally drop raw counts
df = df.drop(['road_count','industrial_count','dump_count','farmland_count'], axis=1)

# -----------------------------
# 5. Feature Scaling
# -----------------------------
from sklearn.preprocessing import MinMaxScaler

scaler_cols = ['pm2_5','pm10','no2','co','so2','o3',
               'temp_day','temp_min','temp_max','humidity','pressure',
               'wind_speed','wind_deg','clouds',
               'road_density','industrial_density','dump_density','farmland_density']

scaler = MinMaxScaler()
df[scaler_cols] = scaler.fit_transform(df[scaler_cols])

# -----------------------------
# 6. Save Preprocessed File
# -----------------------------
df.to_csv("/content/city_air_weather_osm_preprocessed.csv", index=False)

print("Preprocessing Done. Sample data:")
print(df.isnull().sum())

df=pd.read_csv("/content/city_air_weather_osm_preprocessed.csv")
print(df.shape)
print(df.head())

import pandas as pd
import numpy as np

# Load the preprocessed data (Step 1 of the plan)
try:
    df = pd.read_csv("/content/city_air_weather_osm_preprocessed.csv")
    print("Preprocessed data loaded successfully.")
    print(df.head())
except FileNotFoundError:
    print("Error: The file '/content/city_air_weather_osm_preprocessed.csv' was not found.")
    print("Please make sure the preprocessing step was completed successfully and the file exists.")
    df = None # Set df to None to avoid errors in subsequent steps

# Define and apply labeling rules (Steps 2 & 3 of the plan)
if df is not None:
    # Initialize a new column for pollution source labels
    df['pollution_source'] = 'Other' # Default label

    # Define thresholds for 'high' pollution levels (these are examples and may need adjustment)
    # These thresholds should ideally be based on domain knowledge or data distribution analysis
    # Using quantiles on the preprocessed (scaled) data
    threshold_no2 = df['no2'].quantile(0.75) # Example: 75th percentile
    threshold_so2 = df['so2'].quantile(0.75) # Example: 75th percentile
    threshold_pm = df['pm2_5'].quantile(0.75) # Using PM2.5 as a proxy for PM, adjust as needed

    # Apply labeling rules
    # Rule: Close to main road + high NO₂ = Vehicular
    df.loc[(df['road_density'] > df['road_density'].median()) & (df['no2'] > threshold_no2), 'pollution_source'] = 'Vehicular'

    # Rule: Near factory + high SO₂ = Industrial
    df.loc[(df['industrial_density'] > df['industrial_density'].median()) & (df['so2'] > threshold_so2), 'pollution_source'] = 'Industrial'

    # Rule: Near farmland + dry season + high PM = Agricultural
    # Assuming 'dry season' could be inferred from low humidity or high temperature (needs domain knowledge for India)
    # For simplicity here, we'll use a placeholder for 'dry season' or a combination of weather features
    # Let's use low humidity as a simple proxy for dry conditions for this example
    # Using quantile on the preprocessed (scaled) data
    threshold_humidity = df['humidity'].quantile(0.25) # Example: 25th percentile (low humidity)

    df.loc[(df['farmland_density'] > df['farmland_density'].median()) & (df['humidity'] < threshold_humidity) & (df['pm2_5'] > threshold_pm), 'pollution_source'] = 'Agricultural'


    # Display the count of each label
    print("\nPollution Source Label Counts:")
    print(df['pollution_source'].value_counts())

    # Display head of the dataframe with new labels
    print("\nDataFrame with Pollution Source Labels:")
    print(df[['city', 'road_density', 'industrial_density', 'farmland_density', 'no2', 'so2', 'pm2_5', 'humidity', 'pollution_source']].head())

    # You might want to save this labeled dataset for the next steps
    df.to_csv("city_air_weather_osm_labeled.csv", index=False)

    print("\nLabeled dataset saved to 'city_air_weather_osm_labeled.csv'")



import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV # Import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load the labeled dataset (Assuming it's saved from the previous step)
try:
    df = pd.read_csv("/content/city_air_weather_osm_labeled.csv")
    print("Labeled data loaded successfully.")
except FileNotFoundError:
    print("Error: The file '/content/city_air_weather_osm_labeled.csv' was not found.")
    print("Please make sure the labeling step was completed successfully and the file exists.")
    df = None

if df is not None:
    # Prepare features and target variable
    features = df.drop(['city', 'lat', 'lon', 'timestamp', 'city_code', 'pollution_source'], axis=1)
    target = df['pollution_source']

    # Handle potential missing values if any remained after initial preprocessing
    features = features.fillna(features.median())

    # Encode target variable
    y_encoded, target_classes = pd.factorize(target)
    y_encoded = pd.Series(y_encoded)

    # Split data into training and testing sets (using the same split as before)
    X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(features, y_encoded, test_size=0.2, random_state=42)
    print(f"\nData split into training ({X_train.shape[0]} samples) and testing ({X_test.shape[0]} samples) sets.")


    # -----------------------------
    # Hyperparameter Tuning for Random Forest
    # -----------------------------
    print("\nStarting Hyperparameter Tuning for Random Forest...")

    # Define the parameter grid to search
    param_grid_rf = {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }

    # Initialize GridSearchCV
    # Using a smaller cv value (e.g., 3 or 5) for faster execution during initial testing
    grid_search_rf = GridSearchCV(estimator=RandomForestClassifier(random_state=42),
                                  param_grid=param_grid_rf,
                                  cv=3, # Cross-validation folds
                                  scoring='accuracy', # Metric to optimize
                                  n_jobs=-1) # Use all available cores

    # Perform the grid search on the training data
    grid_search_rf.fit(X_train, y_train_encoded)

    print("\nHyperparameter Tuning for Random Forest Complete.")
    print(f"Best parameters found: {grid_search_rf.best_params_}")
    print(f"Best cross-validation accuracy: {grid_search_rf.best_score_:.4f}")

    # -----------------------------
    # Evaluate the Tuned Random Forest Model
    # -----------------------------
    print("\nEvaluating Tuned Random Forest Model...")

    # Get the best model from the grid search
    best_rf_model = grid_search_rf.best_estimator_

    # Make predictions on the test set using the best model
    y_pred_encoded_rf_tuned = best_rf_model.predict(X_test)

    # Evaluate model performance
    print("\nTuned Random Forest Model Evaluation:")
    print(f"Accuracy: {accuracy_score(y_test_encoded, y_pred_encoded_rf_tuned):.4f}")

    print("\nClassification Report:")
    unique_test_labels = np.unique(y_test_encoded)
    target_names_in_test = [target_classes[i] for i in unique_test_labels]
    print(classification_report(y_test_encoded, y_pred_encoded_rf_tuned, labels=unique_test_labels, target_names=target_names_in_test))

    # Confusion Matrix
    cm_rf_tuned = confusion_matrix(y_test_encoded, y_pred_encoded_rf_tuned)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm_rf_tuned, annot=True, fmt='d', cmap='Blues', xticklabels=target_names_in_test, yticklabels=target_names_in_test)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Tuned Random Forest Confusion Matrix')
    plt.show()

!pip install xgboost

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load the labeled dataset (Assuming it's saved from the previous step)
try:
    df = pd.read_csv("/content/city_air_weather_osm_labeled.csv")
    print("Labeled data loaded successfully.")
except FileNotFoundError:
    print("Error: The file '/content/city_air_weather_osm_labeled.csv' was not found.")
    print("Please make sure the labeling step was completed successfully and the file exists.")
    df = None

if df is not None:
    # Prepare features and target variable
    features = df.drop(['city', 'lat', 'lon', 'timestamp', 'city_code', 'pollution_source'], axis=1)
    target = df['pollution_source']

    # Handle potential missing values if any remained after initial preprocessing
    features = features.fillna(features.median())

    # Encode target variable
    y_encoded, target_classes = pd.factorize(target)
    y_encoded = pd.Series(y_encoded)

    # Split data into training and testing sets (using the same split as before)
    X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(features, y_encoded, test_size=0.2, random_state=42)
    print(f"\nData split into training ({X_train.shape[0]} samples) and testing ({X_test.shape[0]} samples) sets.")

    # -----------------------------
    # Hyperparameter Tuning for XGBoost
    # -----------------------------
    print("\nStarting Hyperparameter Tuning for XGBoost...")

    # Define the parameter grid to search for XGBoost
    # Note: This is a basic grid, you might need to expand it based on results
    param_grid_xgb = {
        'n_estimators': [50, 100, 200],
        'learning_rate': [0.01, 0.1, 0.2],
        'max_depth': [3, 5, 7],
        'subsample': [0.8, 1.0],
        'colsample_bytree': [0.8, 1.0]
    }

    # Initialize GridSearchCV for XGBoost
    # Use a smaller cv value (e.g., 3 or 5) for faster execution during initial testing
    grid_search_xgb = GridSearchCV(estimator=XGBClassifier(objective='multi:softprob',
                                                           num_class=len(target_classes),
                                                           random_state=42,
                                                           use_label_encoder=False, # Deprecated in newer versions
                                                           eval_metric='mlogloss'), # Appropriate metric for multi-class
                                   param_grid=param_grid_xgb,
                                   cv=3, # Cross-validation folds
                                   scoring='accuracy', # Metric to optimize
                                   n_jobs=-1) # Use all available cores

    # Perform the grid search on the training data
    grid_search_xgb.fit(X_train, y_train_encoded)

    print("\nHyperparameter Tuning for XGBoost Complete.")
    print(f"Best parameters found: {grid_search_xgb.best_params_}")
    print(f"Best cross-validation accuracy: {grid_search_xgb.best_score_:.4f}")

    # -----------------------------
    # Evaluate the Tuned XGBoost Model
    # -----------------------------
    print("\nEvaluating Tuned XGBoost Model...")

    # Get the best model from the grid search
    best_xgb_model = grid_search_xgb.best_estimator_

    # Make predictions on the test set using the best model
    y_pred_encoded_xgb_tuned = best_xgb_model.predict(X_test)

    # Evaluate model performance
    print("\nTuned XGBoost Model Evaluation:")
    print(f"Accuracy: {accuracy_score(y_test_encoded, y_pred_encoded_xgb_tuned):.4f}")

    print("\nClassification Report:")
    unique_test_labels = np.unique(y_test_encoded)
    target_names_in_test = [target_classes[i] for i in unique_test_labels]
    print(classification_report(y_test_encoded, y_pred_encoded_xgb_tuned, labels=unique_test_labels, target_names=target_names_in_test))

    # Confusion Matrix
    cm_xgb_tuned = confusion_matrix(y_test_encoded, y_pred_encoded_xgb_tuned)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm_xgb_tuned, annot=True, fmt='d', cmap='Blues', xticklabels=target_names_in_test, yticklabels=target_names_in_test)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Tuned XGBoost Confusion Matrix')
    plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier # Import Decision Tree
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load the labeled dataset (Assuming it's saved from the previous step)
try:
    df = pd.read_csv("/content/city_air_weather_osm_labeled.csv")
    print("Labeled data loaded successfully.")
except FileNotFoundError:
    print("Error: The file '/content/city_air_weather_osm_labeled.csv' was not found.")
    print("Please make sure the labeling step was completed successfully and the file exists.")
    df = None

if df is not None:
    # Prepare features and target variable
    features = df.drop(['city', 'lat', 'lon', 'timestamp', 'city_code', 'pollution_source'], axis=1)
    target = df['pollution_source']

    # Handle potential missing values if any remained after initial preprocessing
    features = features.fillna(features.median())

    # Encode target variable
    y_encoded, target_classes = pd.factorize(target)
    y_encoded = pd.Series(y_encoded)

    # Split data into training and testing sets (using the same split as before)
    X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(features, y_encoded, test_size=0.2, random_state=42)
    print(f"\nData split into training ({X_train.shape[0]} samples) and testing ({X_test.shape[0]} samples) sets.")

    # -----------------------------
    # Hyperparameter Tuning for Decision Tree
    # -----------------------------
    print("\nStarting Hyperparameter Tuning for Decision Tree...")

    # Define the parameter grid to search for Decision Tree
    # Note: This is a basic grid, you might need to expand it based on results
    param_grid_dt = {
        'max_depth': [None, 5, 10, 20],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'criterion': ['gini', 'entropy']
    }

    # Initialize GridSearchCV for Decision Tree
    # Use a smaller cv value (e.g., 3 or 5) for faster execution during initial testing
    grid_search_dt = GridSearchCV(estimator=DecisionTreeClassifier(random_state=42),
                                  param_grid=param_grid_dt,
                                  cv=3, # Cross-validation folds
                                  scoring='accuracy', # Metric to optimize
                                  n_jobs=-1) # Use all available cores

    # Perform the grid search on the training data
    grid_search_dt.fit(X_train, y_train_encoded)

    print("\nHyperparameter Tuning for Decision Tree Complete.")
    print(f"Best parameters found: {grid_search_dt.best_params_}")
    print(f"Best cross-validation accuracy: {grid_search_dt.best_score_:.4f}")

    # -----------------------------
    # Evaluate the Tuned Decision Tree Model
    # -----------------------------
    print("\nEvaluating Tuned Decision Tree Model...")

    # Get the best model from the grid search
    best_dt_model = grid_search_dt.best_estimator_

    # Make predictions on the test set using the best model
    y_pred_encoded_dt_tuned = best_dt_model.predict(X_test)

    # Evaluate model performance
    print("\nTuned Decision Tree Model Evaluation:")
    print(f"Accuracy: {accuracy_score(y_test_encoded, y_pred_encoded_dt_tuned):.4f}")

    print("\nClassification Report:")
    unique_test_labels = np.unique(y_test_encoded)
    target_names_in_test = [target_classes[i] for i in unique_test_labels]
    print(classification_report(y_test_encoded, y_pred_encoded_dt_tuned, labels=unique_test_labels, target_names=target_names_in_test))

    # Confusion Matrix
    cm_dt_tuned = confusion_matrix(y_test_encoded, y_pred_encoded_dt_tuned)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm_dt_tuned, annot=True, fmt='d', cmap='Blues', xticklabels=target_names_in_test, yticklabels=target_names_in_test)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Tuned Decision Tree Confusion Matrix')
    plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import re # Import regex module

def extract_accuracy(output_string):
    """Extracts the Accuracy score from a given output string."""
    match = re.search(r"Accuracy: (\d\.\d{4})", output_string)
    if match:
        return float(match.group(1))
    return None

# Dynamically get outputs from previous cells
# Instead of trying to access `latest_notebook` (which is an agent-internal object),
# we will directly use the standard_output strings from the previous cells.

output_rf = """Labeled data loaded successfully.\n\nData split into training (592 samples) and testing (148 samples) sets.\n\nStarting Hyperparameter Tuning for Random Forest...\n\nHyperparameter Tuning for Random Forest Complete.\nBest parameters found: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\nBest cross-validation accuracy: 0.9307\n\nEvaluating Tuned Random Forest Model...\n\nTuned Random Forest Model Evaluation:\nAccuracy: 0.9324\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       Other       0.94      1.00      0.97       120\nAgricultural       1.00      0.20      0.33         5\n  Industrial       0.92      0.79      0.85        14\n   Vehicular       0.86      0.67      0.75         9\n\n    accuracy                           0.93       148\n   macro avg       0.93      0.66      0.72       148\nweighted avg       0.93      0.93      0.92       148\n"""

output_xgb = """Labeled data loaded successfully.\n\nData split into training (592 samples) and testing (148 samples) sets.\n\nStarting Hyperparameter Tuning for XGBoost...\n\n\nHyperparameter Tuning for XGBoost Complete.\nBest parameters found: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}\nBest cross-validation accuracy: 0.9797\n\nEvaluating Tuned XGBoost Model...\n\nTuned XGBoost Model Evaluation:\nAccuracy: 0.9797\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       Other       0.98      1.00      0.99       120\nAgricultural       1.00      0.40      0.57         5\n  Industrial       0.93      1.00      0.97        14\n   Vehicular       1.00      1.00      1.00         9\n\n    accuracy                           0.98       148\n   macro avg       0.98      0.85      0.88       148\nweighted avg       0.98      0.98      0.98       148\n"""

output_dt = """Labeled data loaded successfully.\n\nData split into training (592 samples) and testing (148 samples) sets.\n\nStarting Hyperparameter Tuning for Decision Tree...\n\nHyperparameter Tuning for Decision Tree Complete.\nBest parameters found: {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\nBest cross-validation accuracy: 0.9341\n\nEvaluating Tuned Decision Tree Model...\n\nTuned Decision Tree Model Evaluation:\nAccuracy: 0.9595\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       Other       0.98      1.00      0.99       120\nAgricultural       0.33      0.20      0.25         5\n  Industrial       0.92      0.86      0.89        14\n   Vehicular       0.90      1.00      0.95         9\n\n    accuracy                           0.96       148\n   macro avg       0.79      0.76      0.77       148\nweighted avg       0.95      0.96      0.95       148\n"""

accuracy_rf_tuned = extract_accuracy(output_rf)
accuracy_xgb_tuned = extract_accuracy(output_xgb)
accuracy_dt = extract_accuracy(output_dt)

model_names = ['Decision Tree (Tuned)', 'Random Forest (Tuned)', 'XGBoost (Tuned)']
accuracies = [accuracy_dt, accuracy_rf_tuned, accuracy_xgb_tuned]

# Remove None values if any accuracy extraction failed
valid_accuracies = [(name, acc) for name, acc in zip(model_names, accuracies) if acc is not None]

if not valid_accuracies:
    print("Could not extract any model accuracies. Please ensure model evaluation cells ran successfully.")
else:
    performance_comparison = pd.DataFrame(valid_accuracies, columns=['Model', 'Accuracy'])

    # Sort by accuracy for better visualization
    performance_comparison = performance_comparison.sort_values(by='Accuracy', ascending=False)

    print("Model Performance Comparison (Accuracy):")
    display(performance_comparison)

    # Optional: Visualize the comparison
    plt.figure(figsize=(10, 6))
    # Fix for FutureWarning: Passing `palette` without assigning `hue`
    sns.barplot(x='Accuracy', y='Model', hue='Model', data=performance_comparison, palette='viridis', legend=False)
    plt.xlabel('Accuracy')
    plt.ylabel('Model')
    plt.title('Comparison of Model Accuracies')
    plt.xlim(0, 1.0) # Accuracy is between 0 and 1
    plt.show()

    print("\nBased on Accuracy, the best performing model is:", performance_comparison.iloc[0]['Model'])

"""## Project Libraries and Their Functions

This project utilized several Python libraries for data handling, API interaction, geographical analysis, machine learning, and dashboard development. Below is a breakdown of each library and its key functions:

### 1. Data Acquisition and Handling

*   **`opendatasets`** (`import opendatasets as op`)
    *   **Purpose**: To download datasets directly from Kaggle.
    *   **Key Function**: `op.download()`: Used to download a specified Kaggle dataset into the Colab environment.

*   **`pandas`** (`import pandas as pd`)
    *   **Purpose**: Fundamental library for data manipulation and analysis, especially with tabular data (DataFrames).
    *   **Key Functions**:
        *   `pd.read_csv()`: Reads data from CSV files into a DataFrame.
        *   `DataFrame.head()`: Displays the first few rows of a DataFrame.
        *   `DataFrame.shape`: Returns the dimensions (rows, columns) of a DataFrame.
        *   `DataFrame.fillna()`: Fills missing (NaN) values, typically with a median or mean.
        *   `pd.to_datetime()`: Converts columns to datetime objects.
        *   `astype('category').cat.codes`: Encodes categorical columns into numerical codes.
        *   `DataFrame.drop()`: Removes specified columns or rows.
        *   `DataFrame.to_csv()`: Saves a DataFrame to a CSV file.
        *   `DataFrame.iterrows()`: Iterates over DataFrame rows as (index, Series) pairs.
        *   `DataFrame.quantile()`: Calculates quantiles of numerical data.

*   **`requests`** (`import requests`)
    *   **Purpose**: To make HTTP requests to external APIs (OpenWeatherMap).
    *   **Key Function**: `requests.get()`: Sends an HTTP GET request to a specified URL to retrieve data.

### 2. Geographical Data and Features

*   **`osmnx`** (`import osmnx as ox`)
    *   **Purpose**: To download, model, analyze, and visualize street networks and other OpenStreetMap (OSM) data.
    *   **Key Function**: `ox.features_from_place()`: Fetches OSM features (like roads, industrial areas, farmlands, waste disposal sites) for a given place name.

*   **`folium`** (`import folium`)
    *   **Purpose**: For creating interactive maps that can be embedded into web applications.
    *   **Key Functions**:
        *   `folium.Map()`: Initializes an interactive map centered at specified coordinates.
        *   `folium.Marker()`: Adds a marker to the map at a specific location.
        *   `folium.Icon()`: Customizes the icon for a marker.
        *   `folium.CircleMarker()`: Adds a circle marker to the map, useful for visualizing intensity.
        *   `folium.FeatureGroup()`: Organizes map elements into toggleable layers.
        *   `folium.LayerControl()`: Adds a control to the map to toggle layers on and off.

*   **`folium.plugins.HeatMap`** (`from folium.plugins import HeatMap`)
    *   **Purpose**: To add heatmap layers to Folium maps, visualizing data density.
    *   **Key Function**: `HeatMap().add_to()`: Creates and adds a heatmap layer to a Folium map.

### 3. Machine Learning and Preprocessing

*   **`sklearn.preprocessing.MinMaxScaler`** (`from sklearn.preprocessing import MinMaxScaler`)
    *   **Purpose**: To scale features to a given range (typically 0 to 1).
    *   **Key Functions**:
        *   `MinMaxScaler()`: Initializes the scaler.
        *   `scaler.fit_transform()`: Fits the scaler to data and then transforms it.
        *   `scaler.transform()`: Transforms new data using the fitted scaler.

*   **`sklearn.model_selection`** (`from sklearn.model_selection import train_test_split, GridSearchCV`)
    *   **Purpose**: For splitting data and hyperparameter tuning.
    *   **Key Functions**:
        *   `train_test_split()`: Divides data into training and testing sets.
        *   `GridSearchCV()`: Performs an exhaustive search over specified parameter values for an estimator.

*   **`sklearn.ensemble.RandomForestClassifier`** (`from sklearn.ensemble import RandomForestClassifier`)
    *   **Purpose**: Implements the Random Forest classification algorithm.
    *   **Key Function**: `RandomForestClassifier()`: Initializes and trains a Random Forest model.

*   **`xgboost.XGBClassifier`** (`from xgboost import XGBClassifier`)
    *   **Purpose**: Implements the XGBoost gradient boosting classification algorithm, known for high performance.
    *   **Key Function**: `XGBClassifier()`: Initializes and trains an XGBoost model.

*   **`sklearn.tree.DecisionTreeClassifier`** (`from sklearn.tree import DecisionTreeClassifier`)
    *   **Purpose**: Implements the Decision Tree classification algorithm.
    *   **Key Function**: `DecisionTreeClassifier()`: Initializes and trains a Decision Tree model.

*   **`sklearn.metrics`** (`from sklearn.metrics import accuracy_score, classification_report, confusion_matrix`)
    *   **Purpose**: To evaluate the performance of classification models.
    *   **Key Functions**:
        *   `accuracy_score()`: Calculates the proportion of correctly classified instances.
        *   `classification_report()`: Generates a text report showing the main classification metrics (precision, recall, f1-score, support) per class.
        *   `confusion_matrix()`: Computes the confusion matrix to evaluate the accuracy of a classification.

### 4. Utility and System Operations

*   **`numpy`** (`import numpy as np`)
    *   **Purpose**: Provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions.
    *   **Key Usage**: Often used implicitly by pandas and scikit-learn for numerical computations.

*   **`os`** (`import os`)
    *   **Purpose**: Provides a way of using operating system dependent functionality, like interacting with the file system or environment variables.
    *   **Key Functions**:
        *   `os.path.exists()`: Checks if a file or directory exists.
        *   `os.environ.get()`: Retrieves the value of an environment variable.
        *   `os.environ[...] = `: Sets an environment variable.

*   **`datetime`** (`from datetime import datetime, UTC`)
    *   **Purpose**: To work with dates and times.
    *   **Key Functions**: `datetime.fromtimestamp()`: Converts a Unix timestamp to a datetime object.

*   **`joblib`** (`import joblib`)
    *   **Purpose**: For efficiently saving and loading Python objects, especially large NumPy arrays or scikit-learn models.
    *   **Key Functions**:
        *   `joblib.dump()`: Serializes and saves a Python object to a file.
        *   `joblib.load()`: Deserializes and loads a Python object from a file.

*   **`re`** (`import re`)
    *   **Purpose**: Provides regular expression operations.
    *   **Key Function**: `re.search()`: Scans through a string looking for the first location where a regular expression pattern produces a match.

### 5. Visualization

*   **`matplotlib.pyplot`** (`import matplotlib.pyplot as plt`)
    *   **Purpose**: For creating static, animated, and interactive visualizations.
    *   **Key Functions**:
        *   `plt.figure()`: Creates a new figure.
        *   `plt.xlabel()`, `plt.ylabel()`, `plt.title()`: Sets labels and titles for plots.
        *   `plt.show()`: Displays the plot.

*   **`seaborn`** (`import seaborn as sns`)
    *   **Purpose**: For making statistical graphics based on matplotlib.
    *   **Key Function**: `sns.heatmap()`: Creates a heatmap to visualize a matrix or tabular data (e.g., confusion matrix).
        *   `sns.barplot()`: Creates a bar plot for categorical data.

### 6. Web Application Development

*   **`streamlit`** (`import streamlit as st`)
    *   **Purpose**: For quickly building interactive web applications (dashboards) for data science and machine learning.
    *   **Key Functions**:
        *   `st.set_page_config()`: Configures the Streamlit page (e.g., layout, title).
        *   `st.title()`, `st.header()`, `st.subheader()`, `st.write()`: Functions for adding text elements.
        *   `st.text_input()`, `st.number_input()`, `st.radio()`, `st.button()`: Widgets for user input.
        *   `st.success()`, `st.warning()`, `st.error()`, `st.info()`: Functions for displaying status messages.
        *   `st.json()`, `st.dataframe()`: Displays data in JSON or DataFrame format.
        *   `st.line_chart()`, `st.bar_chart()`: Displays built-in chart types.
        *   `st.components.v1.html()`: Embeds custom HTML content (used for Folium maps).
        *   `st.download_button()`: Creates a button to download data.

*   **`base64`** (`import base64`)
    *   **Purpose**: Provides functions for encoding and decoding binary data using Base64.
    *   **Key Usage**: Used to encode file content for Streamlit's `st.download_button`.

*   **`pyngrok`** (`from pyngrok import ngrok`)
    *   **Purpose**: Python wrapper for ngrok, a tool that creates secure tunnels to expose local web servers to the internet.
    *   **Key Functions**:
        *   `ngrok.set_auth_token()`: Sets the authentication token for ngrok.
        *   `ngrok.connect()`: Creates a public URL for a local port.
        *   `ngrok.kill()`: Kills all running ngrok processes.

*   **`google.colab.userdata`** (`from google.colab import userdata`)
    *   **Purpose**: To securely access user data (like API keys) stored in Colab secrets.
    *   **Key Function**: `userdata.get()`: Retrieves a secret by its name.

## Project Data Overview, Preprocessing, and Final Results

### 1. What Data We Have

The project primarily leverages data from three main sources:

*   **Indian Cities Latitude & Longitude Dataset (Kaggle):** This is the foundational dataset, providing geographical coordinates (latitude and longitude) for various Indian cities. It was downloaded using `opendatasets`.
*   **OpenWeatherMap API:**
    *   **Air Pollution Data:** Real-time and historical air quality metrics (PM2.5, PM10, NO2, CO, SO2, O3) for each city's coordinates.
    *   **Current Weather Data:** Real-time weather conditions (temperature, humidity, pressure, wind speed, wind direction, clouds) for each city's coordinates.
*   **OpenStreetMap (OSM) Data via OSMnx:** Geographical features extracted for each city, including counts of roads (`road_count`), industrial areas (`industrial_count`), waste disposal sites (`dump_count`), and farmlands (`farmland_count`). This data provides contextual information about land use and infrastructure.

### 2. What Data Be Seen After Preprocessing

After raw data collection, a comprehensive preprocessing pipeline transforms the disparate data into a unified, clean, and feature-rich dataset (`city_air_weather_osm_preprocessed.csv` and `city_air_weather_osm_labeled.csv`). The key steps and their impacts are:

*   **Missing Value Handling:** All numerical columns (e.g., pollutant levels, weather metrics, OSM counts) had their missing values imputed using the **median** of their respective columns. This ensures no data is lost and models can be trained on complete features.
*   **Timestamp Conversion:** The raw timestamp from the API calls was converted into a proper `datetime` object, enabling potential time-series analysis (though not extensively used in the final dashboard, it's a standard practice for temporal data).
*   **City Encoding:** City names, originally categorical strings, were encoded into numerical `city_code` using pandas' `factorize`. This is essential for machine learning models that require numerical inputs.
*   **OSM Feature Engineering:** Raw counts of OSM features (roads, industrial, dump, farmland) were converted into **relative densities** by dividing them by the maximum count observed across all cities. This normalizes these features and makes them comparable across different-sized cities (e.g., `road_density`, `industrial_density`). The original raw count columns were then dropped.
*   **Feature Scaling:** All numerical features (pollutants, weather, and engineered OSM densities) were scaled to a range between 0 and 1 using `MinMaxScaler`. This prevents features with larger numerical ranges from dominating the learning process of machine learning models.
*   **Pollution Source Labeling:** A crucial step involved creating a `pollution_source` target variable. This categorical label (`Vehicular`, `Industrial`, `Agricultural`, `Other`) was assigned based on predefined rules using a combination of high pollutant levels (e.g., NO2 for vehicular, SO2 for industrial, PM2.5 for agricultural) and the respective OSM density features (e.g., high NO2 + high road density = Vehicular). This labeled dataset (`city_air_weather_osm_labeled.csv`) is what the classification models were trained on.

### 3. Our Final Result

The project culminates in a machine learning model capable of predicting pollution sources and an interactive Streamlit dashboard for visualization and real-time insights:

*   **Machine Learning Model:**
    *   Three classification models were trained and hyperparameter-tuned: **Decision Tree**, **Random Forest**, and **XGBoost**.
    *   **XGBoost (Tuned)** emerged as the best-performing model with an **Accuracy of 0.9797** and a **Weighted Average F1-Score of 0.98**. This model is saved (`best_pollution_source_model.joblib`) and used for predictions in the dashboard.
*   **Interactive Streamlit Dashboard:**
    *   **Real-time Prediction:** Allows users to input a city name or coordinates to fetch current air pollution and weather data from OpenWeatherMap and get an immediate prediction of the dominant pollution source, along with confidence scores.
    *   **Dynamic Geospatial Map:** Replaces a static map with an interactive Folium map that:
        *   Is centered on the user's queried location.
        *   Displays a **heatmap of historical PM2.5 levels** across various cities.
        *   Shows **historical pollution sources** as color-coded markers (Vehicular, Industrial, Agricultural, Other).
        *   Highlights **historical high PM2.5 zones** with distinct circle markers.
        *   Places a **unique marker at the queried location**, indicating its predicted pollution source and real-time PM2.5 level.
        *   Includes a **Layer Control** for users to toggle different map overlays.
    *   **Historical Data Visualizations:** Presents charts showing pollutant trends over time and the distribution of predicted pollution sources from the historical dataset.
    *   **Real-time Pollutant Levels & Weather Conditions:** Displays the current values of PM2.5, PM10, NO2, etc., along with temperature, humidity, and wind details for the queried location.
    *   **Alerts:** Provides in-dashboard alerts for high PM2.5 levels based on a predefined threshold.
    *   **Report Download:** Allows users to download the historical pollution data as a CSV report.

This project provides a comprehensive tool for understanding, predicting, and visualizing air pollution sources in Indian cities.
"""

import re
import pandas as pd

def extract_f1_score(output_string):
    """Extracts the weighted average F1-score from a classification report string."""
    match = re.search(r"weighted avg\s+\d\.\d{2}\s+\d\.\d{2}\s+(\d\.\d{2})", output_string)
    if match:
        return float(match.group(1))
    return None

# Get the standard_output strings from the model evaluation cells
# These outputs are from the last successful execution of those cells.
output_rf = """Labeled data loaded successfully.\n\nData split into training (592 samples) and testing (148 samples) sets.\n\nStarting Hyperparameter Tuning for Random Forest...\n\nHyperparameter Tuning for Random Forest Complete.\nBest parameters found: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\nBest cross-validation accuracy: 0.9307\n\nEvaluating Tuned Random Forest Model...\n\nTuned Random Forest Model Evaluation:\nAccuracy: 0.9324\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       Other       0.94      1.00      0.97       120\nAgricultural       1.00      0.20      0.33         5\n  Industrial       0.92      0.79      0.85        14\n   Vehicular       0.86      0.67      0.75         9\n\n    accuracy                           0.93       148\n   macro avg       0.93      0.66      0.72       148\nweighted avg       0.93      0.93      0.92       148\n"""

output_xgb = """Labeled data loaded successfully.\n\nData split into training (592 samples) and testing (148 samples) sets.\n\nStarting Hyperparameter Tuning for XGBoost...\n\n\n\nHyperparameter Tuning for XGBoost Complete.\nBest parameters found: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}\nBest cross-validation accuracy: 0.9797\n\nEvaluating Tuned XGBoost Model...\n\nTuned XGBoost Model Evaluation:\nAccuracy: 0.9797\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       Other       0.98      1.00      0.99       120\nAgricultural       1.00      0.40      0.57         5\n  Industrial       0.93      1.00      0.97        14\n   Vehicular       1.00      1.00      1.00         9\n\n    accuracy                           0.98       148\n   macro avg       0.98      0.85      0.88       148\nweighted avg       0.98      0.98      0.98       148\n"""

output_dt = """Labeled data loaded successfully.\n\nData split into training (592 samples) and testing (148 samples) sets.\n\nStarting Hyperparameter Tuning for Decision Tree...\n\nHyperparameter Tuning for Decision Tree Complete.\nBest parameters found: {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\nBest cross-validation accuracy: 0.9341\n\nEvaluating Tuned Decision Tree Model...\n\nTuned Decision Tree Model Evaluation:\nAccuracy: 0.9595\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       Other       0.98      1.00      0.99       120\nAgricultural       0.33      0.20      0.25         5\n  Industrial       0.92      0.86      0.89        14\n   Vehicular       0.90      1.00      0.95         9\n\n    accuracy                           0.96       148\n   macro avg       0.79      0.76      0.77       148\nweighted avg       0.95      0.96      0.95       148\n"""

f1_rf_tuned = extract_f1_score(output_rf)
f1_xgb_tuned = extract_f1_score(output_xgb)
f1_dt_tuned = extract_f1_score(output_dt)

# Also retrieve accuracies for context
accuracy_rf_tuned = extract_accuracy(output_rf)
accuracy_xgb_tuned = extract_accuracy(output_xgb)
accuracy_dt_tuned = extract_accuracy(output_dt)

model_performance = pd.DataFrame({
    'Model': ['Decision Tree (Tuned)', 'Random Forest (Tuned)', 'XGBoost (Tuned)'],
    'Accuracy': [accuracy_dt_tuned, accuracy_rf_tuned, accuracy_xgb_tuned],
    'F1-Score (Weighted Avg)': [f1_dt_tuned, f1_rf_tuned, f1_xgb_tuned]
})

print("\nModel Performance Comparison (Accuracy & F1-Score Weighted Avg):")
display(model_performance.sort_values(by='F1-Score (Weighted Avg)', ascending=False))

import joblib
import os

# Assuming 'best_xgb_model' is the variable holding your trained XGBoost model
# from the hyperparameter tuning step (cell 26331222).
# Make sure to run that cell first to have the best_xgb_model available.

# Define the filename for the exported model
model_filename = 'best_pollution_source_model.joblib'

try:
    # Export the model using joblib
    joblib.dump(best_xgb_model, model_filename)

    print(f"Tuned XGBoost model successfully exported to '{model_filename}'")

    # Verify the file exists
    if os.path.exists(model_filename):
        print(f"File '{model_filename}' created successfully.")
except NameError:
    print("Error: 'best_xgb_model' is not defined.")
    print("Please make sure you have run the XGBoost hyperparameter tuning cell (cell 26331222) successfully.")
except Exception as e:
    print(f"An error occurred during model export: {e}")

import pandas as pd
import folium
from folium.plugins import HeatMap
import os

# --- Step 1 & 2: Load and Prepare Data ---
# We'll load the labeled dataset which contains location and pollution data,
# and the 'pollution_source' label.
try:
    df = pd.read_csv("/content/city_air_weather_osm_labeled.csv")
    print("Labeled data loaded successfully for mapping.")
    # Ensure necessary columns are present
    if not all(col in df.columns for col in ['lat', 'lon', 'pm2_5', 'pollution_source']):
        print("Error: Required columns (lat, lon, pm2_5, pollution_source) not found in the dataset.")
        df = None # Set df to None to prevent further errors
except FileNotFoundError:
    print("Error: The file '/content/city_air_weather_osm_labeled.csv' was not found.")
    print("Please make sure the labeling step was completed successfully and the file exists.")
    df = None


if df is not None:
    # --- Step 3: Initialize Map ---
    # Find the center of India for map initialization
    # Using the mean latitude and longitude from the dataset
    map_center_lat = df['lat'].mean()
    map_center_lon = df['lon'].mean()

    # Create a base map using Folium
    india_map = folium.Map(location=[map_center_lat, map_center_lon], zoom_start=5)
    print("Map initialized.")

    # --- Step 4: Create Pollution Heatmap (using PM2.5 as an example) ---
    print("Adding Heatmap layer...")
    # Prepare data for heatmap: list of [latitude, longitude, value]
    # Using PM2.5 for the heatmap intensity
    heatmap_data = df[['lat', 'lon', 'pm2_5']].values.tolist()

    # Add HeatMap layer to the map
    HeatMap(heatmap_data).add_to(india_map)
    print("Heatmap layer added.")


    # --- Step 5: Add Source-Specific Markers ---
    print("Adding source-specific markers...")

    # Define colors or icons for different pollution sources
    source_colors = {
        'Vehicular': 'blue',
        'Industrial': 'red',
        'Agricultural': 'green',
        'Other': 'gray' # Default color
    }

    # Add a layer for markers so they can be toggled on/off
    marker_layer = folium.FeatureGroup(name='Pollution Sources').add_to(india_map)

    # Iterate through each city and add a marker
    for index, row in df.iterrows():
        city_name = row['city']
        latitude = row['lat']
        longitude = row['lon']
        pollution_source = row['pollution_source']
        pm2_5_value = row['pm2_5']

        # Get the color based on the pollution source
        marker_color = source_colors.get(pollution_source, 'gray') # Use gray for unknown sources

        # Create a popup with information
        popup_text = f"<b>City:</b> {city_name}<br>" \
                     f"<b>Predicted Source:</b> {pollution_source}<br>" \
                     f"<b>PM2.5:</b> {pm2_5_value:.2f}" # Format PM2.5 value


        # Add a marker to the marker layer
        folium.Marker(
            location=[latitude, longitude],
            popup=popup_text,
            icon=folium.Icon(color=marker_color, icon='info-sign') # Using a simple icon with color
        ).add_to(marker_layer)

    print("Source-specific markers added.")


    # --- Step 6: Visualize High-Risk Zones (Basic Implementation) ---
    # A simple way to visualize higher risk areas is to use different marker sizes or colors
    # based on pollutant severity. We already used color for source, let's use size (radius)
    # for PM2.5 severity on a separate layer.

    print("Adding High PM2.5 zones visualization...")

    high_pm_layer = folium.FeatureGroup(name='High PM2.5 Zones').add_to(india_map)

    # Define a simple rule for "high PM2.5" - e.g., above the 75th percentile of the scaled data
    pm2_5_threshold = df['pm2_5'].quantile(0.75)

    for index, row in df.iterrows():
        latitude = row['lat']
        longitude = row['lon']
        pm2_5_value = row['pm2_5']

        # If PM2.5 is above the threshold, add a larger circle marker
        if pm2_5_value > pm2_5_threshold:
             folium.CircleMarker(
                location=[latitude, longitude],
                radius=10, # Larger radius for high PM2.5
                color='orange',
                fill=True,
                fill_color='orange',
                fill_opacity=0.6,
                popup=f"<b>High PM2.5:</b> {pm2_5_value:.2f}"
            ).add_to(high_pm_layer)

    print("High PM2.5 zones visualization added.")


    # --- Step 7: Add Interactive Controls (Basic Layer Control) ---
    # Add a layer control to toggle layers
    folium.LayerControl().add_to(india_map)
    print("Layer control added.")


    # --- Step 8: Export Map ---
    output_map_file = "pollution_map.html"
    india_map.save(output_map_file)
    print(f"\nInteractive map saved to '{output_map_file}'")
    print(f"You can view the map by downloading '{output_map_file}' from the Files tab and opening it in a web browser.")

    # --- Step 9: Finish Task (Implicit in the output file) ---
    print("\nModule 5 tasks completed in this code block.")
    print("The generated map file visualizes the pollution data and predicted sources.")

# Set your OpenWeatherMap API key in Colab
os.environ["OWM_API_KEY"] = "13bd8cad6b6a28245eff4047fb57163f"

!pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import joblib
# import os
# import requests
# import numpy as np
# from sklearn.preprocessing import MinMaxScaler
# 
# st.set_page_config(page_title="Air Pollution Dashboard", layout="wide")
# 
# st.title("Indian Cities Air Pollution Dashboard")
# st.write("Explore real-time air quality, predicted pollution sources, and historical insights.")
# 
# # -----------------------------
# # Load model
# # -----------------------------
# model_path = "best_pollution_source_model.joblib"
# if os.path.exists(model_path):
#     model = joblib.load(model_path)
#     st.success("✅ Model loaded successfully!")
# else:
#     st.warning(f"⚠️ Model file '{model_path}' not found. Prediction will not work.")
#     model = None
# 
# # -----------------------------
# # Load historical data
# # -----------------------------
# data_path = "/content/city_air_weather_osm_labeled.csv"
# if os.path.exists(data_path):
#     df = pd.read_csv(data_path)
#     st.success("✅ Historical data loaded!")
# else:
#     st.warning(f"⚠️ Data file '{data_path}' not found. Some features will use default values.")
#     df = None
# 
# # -----------------------------
# # Scaler
# # -----------------------------
# scaler_cols = ['pm2_5','pm10','no2','co','so2','o3',
#                'temp_day','temp_min','temp_max','humidity','pressure',
#                'wind_speed','wind_deg','clouds',
#                'road_density','industrial_density','dump_density','farmland_density']
# 
# if df is not None:
#     scaler = MinMaxScaler()
#     try:
#         features_for_scaler = df.drop(['city','lat','lon','timestamp','city_code','pollution_source'], axis=1)
#         features_for_scaler = features_for_scaler.fillna(features_for_scaler.median())
#         scaler.fit(features_for_scaler[scaler_cols])
#     except:
#         st.warning("Could not fit scaler, predictions may fail.")
#         scaler = None
# else:
#     scaler = None
# 
# # -----------------------------
# # OpenWeatherMap API Key
# # -----------------------------
# OWM_KEY = "13bd8cad6b6a28245eff4047fb57163f"
# OWM_KEY = st.secrets.get("OWM_API_KEY", None)
# if not OWM_KEY:
#     st.warning("⚠️ OpenWeatherMap API key not found. Real-time data unavailable.")
# 
# # -----------------------------
# # API Functions
# # -----------------------------
# def get_air_quality(lat, lon):
#     if not OWM_KEY: return None
#     try:
#         url = f"http://api.openweathermap.org/data/2.5/air_pollution?lat={lat}&lon={lon}&appid={OWM_KEY}"
#         return requests.get(url).json()
#     except:
#         return None
# 
# def get_weather(lat, lon):
#     if not OWM_KEY: return None
#     try:
#         url = f"http://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={OWM_KEY}&units=metric"
#         return requests.get(url).json()
#     except:
#         return None
# 
# # -----------------------------
# # OSM Feature Helper
# # -----------------------------
# def get_osm(city_name):
#     if df is not None:
#         city_data = df[df['city'].str.lower() == city_name.lower()]
#         if not city_data.empty:
#             return city_data[['road_density','industrial_density','dump_density','farmland_density']].iloc[0].to_dict()
#     return {'road_density':0,'industrial_density':0,'dump_density':0,'farmland_density':0}
# 
# # -----------------------------
# # User input
# # -----------------------------
# st.header("Enter City or Coordinates")
# input_type = st.radio("Input method:", ["City Name", "Latitude & Longitude"])
# 
# lat = lon = None
# city_name = None
# 
# if input_type == "City Name":
#     city_name = st.text_input("City Name (e.g., Delhi)")
#     if city_name and df is not None:
#         coords = df[df['city'].str.lower()==city_name.lower()][['lat','lon']]
#         if not coords.empty:
#             lat, lon = coords.iloc[0]
#             st.info(f"Coordinates: {lat}, {lon}")
# else:
#     lat = st.number_input("Latitude", format="%.6f")
#     lon = st.number_input("Longitude", format="%.6f")
# 
# # -----------------------------
# # Prediction
# # -----------------------------
# if st.button("Get Pollution Data & Predict") and lat is not None and lon is not None:
# 
#     st.subheader("Real-time Data & Prediction")
# 
#     air_data = get_air_quality(lat, lon)
#     weather_data = get_weather(lat, lon)
# 
#     if air_data and weather_data and model and scaler:
#         try:
#             p = air_data['list'][0]['components']
#             w = weather_data['main']
#             wind = weather_data['wind']
#             clouds = weather_data['clouds']
# 
#             osm = get_osm(city_name if city_name else "Unknown")
# 
#             feat = {
#                 'pm2_5': p.get('pm2_5',0),
#                 'pm10': p.get('pm10',0),
#                 'no2': p.get('no2',0),
#                 'co': p.get('co',0),
#                 'so2': p.get('so2',0),
#                 'o3': p.get('o3',0),
#                 'temp_day': w.get('temp',0),
#                 'temp_min': w.get('temp_min',w.get('temp',0)),
#                 'temp_max': w.get('temp_max',w.get('temp',0)),
#                 'humidity': w.get('humidity',0),
#                 'pressure': w.get('pressure',0),
#                 'wind_speed': wind.get('speed',0),
#                 'wind_deg': wind.get('deg',0),
#                 'clouds': clouds.get('all',0),
#                 'road_density': osm.get('road_density',0),
#                 'industrial_density': osm.get('industrial_density',0),
#                 'dump_density': osm.get('dump_density',0),
#                 'farmland_density': osm.get('farmland_density',0)
#             }
# 
#             feat_df = pd.DataFrame([feat])
#             feat_scaled = scaler.transform(feat_df[scaler_cols])
#             pred_enc = model.predict(feat_scaled)[0]
#             pred_proba = model.predict_proba(feat_scaled)[0]
#             pred_label = model.classes_[pred_enc]
# 
#             st.success(f"Predicted Pollution Source: {pred_label}")
#             st.write("Confidence Scores:")
#             st.dataframe(pd.DataFrame({'Source': model.classes_, 'Confidence': pred_proba}))
# 
#             st.subheader("Real-time Pollutant Levels")
#             st.json({k:v for k,v in p.items() if k in ['pm2_5','pm10','no2','co','so2','o3']})
# 
#         except Exception as e:
#             st.error(f"Error during prediction: {e}")
#     else:
#         st.warning("Data/model unavailable. Check API key, model file, and data file.")
# 
# # -----------------------------
# # Placeholders for charts/map
# # -----------------------------
# st.header("Pollutant Trends Over Time (Coming Soon)")
# st.line_chart(pd.DataFrame({'pm2_5':[0,1,2],'pm10':[0,1,2]}))
# 
# st.header("Predicted Source Distribution (Coming Soon)")
# st.bar_chart(pd.DataFrame({'Industrial':[1,2,3],'Vehicular':[3,2,1]}))
# 
# st.header("Geospatial Map (Coming Soon)")
# st.map(pd.DataFrame({'lat':[lat if lat else 28],'lon':[lon if lon else 77]}))

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import joblib
# import os
# import requests
# from datetime import datetime, timezone
# import numpy as np
# from sklearn.preprocessing import MinMaxScaler
# import time
# import base64 # To encode file for download
# 
# 
# # -----------------------------
# # Load the trained model
# # -----------------------------
# model_filename = 'best_pollution_source_model.joblib'
# model = None
# if os.path.exists(model_filename):
#     try:
#         model = joblib.load(model_filename)
#         # st.success("Trained model loaded successfully!")
#     except Exception as e:
#         st.error(f"Error loading model file '{model_filename}': {e}")
# else:
#     st.error(f"Error: Model file '{model_filename}' not found.")
# 
# 
# # -----------------------------
# # Load historical data for scaler fitting and historical views
# # -----------------------------
# data_filename = '/content/city_air_weather_osm_labeled.csv' # Use the labeled data for historical views and scaler fitting
# df_historical = None
# scaler = None
# if os.path.exists(data_filename):
#     try:
#         df_historical = pd.read_csv(data_filename)
#         # st.success("Historical data loaded successfully!")
# 
#         # Fit scaler on historical data
#         scaler_cols = ['pm2_5','pm10','no2','co','so2','o3',
#                        'temp_day','temp_min','temp_max','humidity','pressure',
#                        'wind_speed','wind_deg','clouds',
#                        'road_density','industrial_density','dump_density','farmland_density']
# 
#         # Ensure columns exist before selecting
#         cols_to_scale = [col for col in scaler_cols if col in df_historical.columns]
#         if cols_to_scale:
#             scaler = MinMaxScaler()
#             features_for_scaler_fit = df_historical[cols_to_scale].fillna(df_historical[cols_to_scale].median())
#             scaler.fit(features_for_scaler_fit)
#             # st.success("Scaler fitted successfully on historical data!")
#         else:
#             st.warning("No valid columns found for scaler fitting.")
#             scaler = None
# 
#     except Exception as e:
#         st.warning(f"Could not load and fit scaler from historical data: {e}")
#         df_historical = None
#         scaler = None
# else:
#     st.warning(f"Warning: Historical data file '{data_filename}' not found. Some features may be limited.")
#     df_historical = None
#     scaler = None
# 
# 
# # -----------------------------
# # OpenWeatherMap API Key Handling
# # -----------------------------
# # Read OWM_KEY from environment variable set in the Colab notebook
# OWM_KEY = os.environ.get("OWM_API_KEY") # Read from environment variable
# 
# if not OWM_KEY:
#     st.warning("OpenWeatherMap API key environment variable not set.")
#     # Add a text input for the user to enter the API key in the dashboard
#     OWM_KEY = st.text_input("Enter your OpenWeatherMap API Key:")
#     if OWM_KEY:
#         # If the user enters the key, set it as an environment variable for the current session
#         os.environ["OWM_API_KEY"] = OWM_KEY
#         st.success("OpenWeatherMap API Key set from input!")
#     else:
#         st.info("Please enter your OpenWeatherMap API Key to fetch real-time data.")
# 
# # -----------------------------
# # API functions
# # -----------------------------
# def get_air_quality(lat, lon, api_key):
#     if not api_key: return None
#     url = f"http://api.openweathermap.org/data/2.5/air_pollution?lat={lat}&lon={lon}&appid={api_key}"
#     try:
#         resp = requests.get(url)
#         resp.raise_for_status()
#         return resp.json()
#     except requests.exceptions.RequestException as e:
#         st.error(f"Error fetching air quality data: {e}")
#         return None
# 
# def get_weather(lat, lon, api_key):
#     if not api_key: return None
#     url = f"http://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={api_key}&units=metric"
#     try:
#         resp = requests.get(url)
#         resp.raise_for_status()
#         return resp.json()
#     except requests.exceptions.RequestException as e:
#         st.error(f"Error fetching weather data: {e}")
#         return None
# 
# # -----------------------------
# # Get OSM features from historical data (using preprocessed data)
# # -----------------------------
# def get_osm_features_from_data(city_name, df_historical):
#     if df_historical is not None:
#         city_data = df_historical[df_historical['city'].str.lower() == city_name.lower()]
#         if not city_data.empty:
#             # Ensure columns exist before selecting
#             osm_cols = ['road_density', 'industrial_density', 'dump_density', 'farmland_density']
#             present_osm_cols = [col for col in osm_cols if col in city_data.columns]
#             if present_osm_cols:
#                  return city_data[present_osm_cols].iloc[0].to_dict()
#     return {
#         'road_density': None,
#         'industrial_density': None,
#         'dump_density': None,
#         'farmland_density': None
#     }
# 
# 
# # -----------------------------
# # Function to generate a sample report (CSV)
# # -----------------------------
# def generate_report(data):
#     # This is a placeholder. In a real application, you would format
#     # the data and include relevant insights or summaries.
#     if data is not None:
#         return data.to_csv(index=False).encode('utf-8')
#     return None
# 
# # -----------------------------
# # Streamlit UI
# # -----------------------------
# st.set_page_config(layout="wide") # Use wide layout
# st.title("Indian Cities Air Pollution Dashboard")
# st.write("Explore real-time air quality data, predicted pollution sources, and historical insights.")
# 
# st.header("Enter City or Coordinates")
# input_option = st.radio("Choose input method:", ("City Name", "Latitude and Longitude"))
# 
# city_name = None
# lat = None
# lon = None
# 
# if input_option == "City Name":
#     city_name = st.text_input("Enter City Name (e.g., Delhi, Mumbai)")
#     if city_name and df_historical is not None:
#         city_coords = df_historical[df_historical['city'].str.lower() == city_name.lower()][['lat', 'lon']]
#         if not city_coords.empty:
#             lat = city_coords.iloc[0]['lat']
#             lon = city_coords.iloc[0]['lon']
#             st.write(f"Coordinates: Latitude {lat:.4f}, Longitude {lon:.4f}")
#         else:
#             st.warning("City not found in historical data. Try Lat/Lon input.")
#             city_name = None
#     elif city_name and df_historical is None:
#         st.warning("Historical data not loaded. Cannot resolve city name.")
#         city_name = None
# else:
#     lat = st.number_input("Enter Latitude", format="%.6f")
#     lon = st.number_input("Enter Longitude", format="%.6f")
# 
# # -----------------------------
# # Fetch data and predict
# # -----------------------------
# if (lat is not None and lon is not None):
#     if st.button("Get Pollution Data and Predict Source"):
#         st.header("Real-time Data and Prediction")
# 
#         # Check if API key is available before fetching data
#         if OWM_KEY:
#             air_data = get_air_quality(lat, lon, OWM_KEY)
#             weather_data = get_weather(lat, lon, OWM_KEY)
# 
#             if air_data and weather_data and model and scaler and df_historical is not None:
#                 try:
#                     p = air_data['list'][0]['components']
#                     w_main = weather_data['main']
#                     w_wind = weather_data['wind']
#                     w_clouds = weather_data['clouds']
# 
#                     osm_features = get_osm_features_from_data(city_name if city_name else "Unknown City", df_historical)
# 
#                     feature_vector_data = {
#                         'pm2_5': p.get('pm2_5'),
#                         'pm10': p.get('pm10'),
#                         'no2': p.get('no2'),
#                         'co': p.get('co'),
#                         'so2': p.get('so2'),
#                         'o3': p.get('o3'),
#                         'temp_day': w_main.get('temp'),
#                         'temp_min': w_main.get('temp_min',w_main.get('temp',0)),
#                         'temp_max': w_main.get('temp_max',w_main.get('temp',0)),
#                         'humidity': w_main.get('humidity'),
#                         'pressure': w_main.get('pressure'),
#                         'wind_speed': w_wind.get('speed'),
#                         'wind_deg': w_wind.get('deg', 0),
#                         'clouds': w_clouds.get('all'),
#                         'road_density': osm_features.get('road_density', df_historical['road_density'].median() if df_historical is not None and 'road_density' in df_historical.columns else None),
#                         'industrial_density': osm_features.get('industrial_density', df_historical['industrial_density'].median() if df_historical is not None and 'industrial_density' in df_historical.columns else None),
#                         'dump_density': osm_features.get('dump_density', df_historical['dump_density'].median() if df_historical is not None and 'dump_density' in df_historical.columns else None),
#                         'farmland_density': osm_features.get('farmland_density', df_historical['farmland_density'].median() if df_historical is not None and 'farmland_density' in df_historical.columns else None)
#                     }
# 
#                     feature_df = pd.DataFrame([feature_vector_data])
# 
#                     # Ensure columns expected by scaler exist and handle NaNs before scaling
#                     cols_to_scale = [col for col in scaler_cols if col in feature_df.columns]
#                     if cols_to_scale:
#                          feature_df[cols_to_scale] = feature_df[cols_to_scale].fillna(df_historical[cols_to_scale].median() if df_historical is not None and all(col in df_historical.columns for col in cols_to_scale) else 0)
#                          scaled_features = scaler.transform(feature_df[cols_to_scale])
#                          scaled_features_df = pd.DataFrame(scaled_features, columns=cols_to_scale)
# 
#                          # Make prediction
#                          prediction_label_encoded = model.predict(scaled_features_df)[0]
#                          prediction_proba = model.predict_proba(scaled_features_df)[0]
# 
#                          # Decode the predicted label
#                          # Need access to target_classes from training - assuming 'Other', 'Industrial', 'Vehicular', 'Agricultural' order
#                          # In a real app, save and load target_classes or use a LabelEncoder
#                          # For this example, we'll hardcode based on previous output
#                          target_classes = ['Other', 'Industrial', 'Vehicular', 'Agricultural']
#                          prediction_label = target_classes[prediction_label_encoded]
# 
# 
#                          st.subheader("Prediction Results")
#                          st.write(f"Predicted Pollution Source: **{prediction_label}**")
# 
#                          st.write("Confidence Scores:")
#                          confidence_df = pd.DataFrame({'Source': target_classes, 'Confidence': prediction_proba})
#                          st.dataframe(confidence_df.sort_values(by='Confidence', ascending=False))
# 
#                          st.subheader("Real-time Pollutant Levels")
#                          current_pollutants = {k: v for k, v in p.items() if k in ['pm2_5', 'pm10', 'no2', 'co', 'so2', 'o3']}
#                          st.json(current_pollutants)
# 
#                          st.subheader("Alerts")
#                          PM2_5_ALERT_THRESHOLD = 50 # Example threshold for PM2.5
#                          pm2_5_level = p.get('pm2_5')
#                          if pm2_5_level is not None and pm2_5_level > PM2_5_ALERT_THRESHOLD:
#                              st.warning(f"🚨 ALERT: PM2.5 level ({pm2_5_level:.2f}) is high!")
# 
#                          else:
#                              st.info("✅ PM2.5 within safe threshold.")
# 
#                          st.subheader("Weather Conditions")
#                          st.write(f"Temperature: {w_main.get('temp', 0):.1f} °C")
#                          st.write(f"Humidity: {w_main.get('humidity', 0)}%")
#                          st.write(f"Wind Speed: {w_wind.get('speed', 0):.1f} m/s")
#                          st.write(f"Wind Direction: {w_wind.get('deg', 0)}°")
#                          st.write(f"Clouds: {w_clouds.get('all', 0)}%")
# 
# 
#                     else:
#                         st.error("Could not prepare features for scaling. Check column names and historical data.")
# 
# 
#                 except Exception as e:
#                     st.error(f"Error during data processing or prediction: {e}")
#             else:
#                 st.warning("Model, scaler, historical data, or OpenWeatherMap API key missing. Cannot make prediction.")
#         else:
#              st.info("Please enter your OpenWeatherMap API Key above to fetch real-time data and make a prediction.")
# 
# 
# # -----------------------------
# # Charts and Visualizations
# # -----------------------------
# st.header("Historical Data Visualizations")
# 
# if df_historical is not None:
#     st.subheader("Pollutant Trends Over Time (Sample)")
#     # This is a placeholder. You would need to aggregate df_historical
#     # by time (e.g., daily, weekly) and select specific pollutants.
#     # Example: df_daily_avg = df_historical.resample('D', on='timestamp').mean()
#     # st.line_chart(df_daily_avg[['pm2_5', 'pm10']])
#     # Ensure 'timestamp' is datetime and set as index for plotting
#     try:
#         df_historical['timestamp'] = pd.to_datetime(df_historical['timestamp'])
#         st.line_chart(df_historical[['timestamp', 'pm2_5', 'pm10']].set_index('timestamp'))
#     except KeyError:
#         st.warning("Timestamp column not found in historical data.")
#     except Exception as e:
#         st.warning(f"Error plotting historical data: {e}")
# 
# 
#     st.subheader("Predicted Source Distribution (Historical)")
#     # This uses the pollution_source column from the historical data
#     if 'pollution_source' in df_historical.columns:
#         source_counts = df_historical['pollution_source'].value_counts()
#         st.bar_chart(source_counts) # Using bar chart as Streamlit doesn't have native pie chart
#     else:
#         st.info("Pollution source labels not available in historical data for distribution chart.")
# 
# 
# else:
#     st.info("Historical data not loaded. Cannot display historical trends and distributions.")
# 
# 
# # -----------------------------
# # Geospatial Map
# # -----------------------------
# st.header("Pollution Map")
# map_file = "pollution_map.html"
# if os.path.exists(map_file):
#     st.write("Interactive map showing pollution data and predicted sources.")
#     # Embed the HTML map file
#     with open(map_file, "r") as f:
#         html_string = f.read()
#         st.components.v1.html(html_string, width=800, height=600, scrolling=True)
# else:
#     st.warning(f"Map file '{map_file}' not found. Please generate the map first.")
# 
# 
# # -----------------------------
# # Alerts Configuration
# # -----------------------------
# # st.header("Alerts")
# # st.write("Receive in-dashboard alerts for critical pollution levels.")
# 
# # -----------------------------
# # Download Reports
# # -----------------------------
# st.header("Download Reports")
# if df_historical is not None:
#     st.write("Download a CSV report of the historical pollution data.")
#     report_data = df_historical # Or process df_historical for a specific report format
#     csv_report = generate_report(report_data)
# 
#     if csv_report:
#         st.download_button(
#             label="Download Historical Data CSV",
#             data=csv_report,
#             file_name='historical_pollution_report.csv',
#             mime='text/csv'
#         )
#     else:
#         st.info("Historical data not available to generate report.")
# else:
#      st.info("Historical data not loaded. Cannot generate reports.")

import os

# Replace "YOUR_OPENWEATHERMAP_API_KEY" with your actual OpenWeatherMap API key
os.environ["OWM_API_KEY"] = "13bd8cad6b6a28245eff4047fb57163f"

print("OpenWeatherMap API key environment variable set.")

!pip install pyngrok --quiet

import threading, time
from pyngrok import ngrok
from google.colab import userdata
import os

# Get the ngrok authtoken from Colab secrets
NGROK_AUTH_TOKEN = userdata.get("NGROK_AUTH_TOKEN")
if NGROK_AUTH_TOKEN:
    ngrok.set_auth_token(NGROK_AUTH_TOKEN)
    print("ngrok authtoken set.")
else:
    print("ngrok authtoken not found in Colab secrets.")
    print("Please add your ngrok authtoken to Colab secrets with the name NGROK_AUTH_TOKEN.")
    # You might want to exit or handle this case further if the token is required


# Set OpenWeatherMap API Key Environment Variable
# Read OWM_API_KEY from environment variable or directly set it here
# IMPORTANT: Replace "YOUR_OPENWEATHERMAP_API_KEY" with your actual key
# We will also pass this directly to the streamlit process
OWM_API_KEY = os.environ.get("OWM_API_KEY", "13bd8cad6b6a28245eff4047fb57163f") # Fallback to hardcoded if env var not set
os.environ["OWM_API_KEY"] = OWM_API_KEY # Ensure it's in the current process environment

# --- NEW: Kill any existing ngrok tunnels before starting a new one ---
ngrok.kill()
print("Killed any existing ngrok processes.")
# -------------------------------------------------------------------


# Start Streamlit in background thread, passing the OWM_API_KEY environment variable
def run_streamlit():
    # Pass environment variable directly to the shell command
    !OWM_API_KEY=$OWM_API_KEY streamlit run app.py --server.port 8501 --server.headless true

thread = threading.Thread(target=run_streamlit)
thread.start()

time.sleep(10)  # Wait for Streamlit to boot

# Start ngrok tunnel
try:
    public_url = ngrok.connect(8501)
    print("✅ Dashboard live at:", public_url)
except Exception as e:
    print(f"Error starting ngrok tunnel: {e}")



from google.colab import drive
drive.mount('/content/drive')