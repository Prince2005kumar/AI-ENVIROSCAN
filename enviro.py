# -*- coding: utf-8 -*-
"""enviro.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DJnZZ3xdfFYbsxuJteRHBqZmxgKnIeoT
"""

!pip install opendatasets

import opendatasets as op
op.download("https://www.kaggle.com/datasets/arjunprasadsarkhel/indian-city-latitude-longitude")

import pandas as pd
df = pd.read_csv("/content/indian-city-latitude-longitude/India Cities LatLng.csv")
df.shape
df.head()

import pandas as pd
import requests
from datetime import datetime

# -----------------------------
# Step 1: Load the city dataset
# -----------------------------
# Corrected file path based on directory listing
df = pd.read_csv("/content/indian-city-latitude-longitude/India Cities LatLng.csv")

# Let's pick first 5 cities for testing
test_cities = df.head(3)  # you can change the number later

# -----------------------------
# Step 2: Your OpenWeatherMap API Key
# -----------------------------
OWM_KEY = "13bd8cad6b6a28245eff4047fb57163f"  # <<--- Replace with your OpenWeatherMap API key

# -----------------------------
# Step 3: Function to call Air Pollution API
# -----------------------------
def fetch_air_pollution(lat, lon, api_key):
    url = f"http://api.openweathermap.org/data/2.5/air_pollution?lat={lat}&lon={lon}&appid={api_key}"
    resp = requests.get(url)
    if resp.status_code != 200:
        print(f"Error fetching data for {lat}, {lon}: {resp.status_code}")
        return None
    data = resp.json()
    return data

# -----------------------------
# Step 4: Test the API for a few cities
# -----------------------------
for idx, row in test_cities.iterrows():
    city = row['city']        # adjust column name if different
    lat = row['lat']     # adjust column name if different
    lon = row['lng']    # adjust column name if different

    print(f"\nFetching air pollution data for {city} ({lat}, {lon})...")
    api_data = fetch_air_pollution(lat, lon, OWM_KEY)

    if api_data:
        # Display main fields from API
        for item in api_data.get('list', []):
            dt = datetime.utcfromtimestamp(item['dt'])
            components = item['components']
            print(f"Timestamp (UTC): {dt}")
            print(f"PM2.5: {components.get('pm2_5')}, PM10: {components.get('pm10')}, NO2: {components.get('no2')}, CO: {components.get('co')}, SO2: {components.get('so2')}, O3: {components.get('o3')}")

!pip install osmnx

import pandas as pd
import requests
from datetime import datetime, UTC
import osmnx as ox

from shapely.geometry import Polygon

# -----------------------------
# Load city dataset
# -----------------------------
df = pd.read_csv("/content/indian-city-latitude-longitude/India Cities LatLng.csv")
cities = df.head(100)   # Process all cities

# -----------------------------
# API Key
# -----------------------------
API_KEY = "13bd8cad6b6a28245eff4047fb57163f"   # <-- replace with your API key

# -----------------------------
# Fetch Air Pollution Data
# -----------------------------
def get_air_quality(lat, lon):
    url = f"http://api.openweathermap.org/data/2.5/air_pollution?lat={lat}&lon={lon}&appid={API_KEY}"
    resp = requests.get(url)
    # print(f"Air pollution API response status code for {lat}, {lon}: {resp.status_code}") # Removed this line
    if resp.status_code == 200:
        return resp.json()
    return None

# -----------------------------
# Fetch Climate/Weather Data
# -----------------------------
def get_weather(lat, lon):
    # Changed to current weather endpoint for testing
    url = f"http://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={API_KEY}&units=metric"
    resp = requests.get(url)
    # print(f"Weather API response status code for {lat}, {lon}: {resp.status_code}") # Removed this line
    if resp.status_code == 200:
        return resp.json()
    return None

# -----------------------------
# Fetch OSM Features (roads, industries, dump sites, farmland)
# -----------------------------




def get_osm_features(city_name):
    features = {"road_count": None, "industrial_count": None, "dump_count": None, "farmland_count": None}
    try:
        # Define tags for the features we want to retrieve
        tags = {"highway": True,
                "landuse": ["industrial", "farmland"],
                "amenity": "waste_disposal"}

        # Fetch features using ox.features_from_place
        osm_features = ox.features_from_place(city_name, tags)

        # Count the features by type
        features["road_count"] = len(osm_features[osm_features.geom_type.isin(['LineString', 'MultiLineString'])]) # Roads are linestrings
        features["industrial_count"] = len(osm_features[osm_features['landuse'] == 'industrial'])
        features["dump_count"] = len(osm_features[osm_features['amenity'] == 'waste_disposal'])
        features["farmland_count"] = len(osm_features[osm_features['landuse'] == 'farmland'])


    except Exception as e:
        # print(f"OSM fetch failed for {city_name}: {e}")
        pass # Added pass statement to fix IndentationError
    return features



# -----------------------------
# Collect Data for Cities
# -----------------------------
all_data = []

for _, row in cities.iterrows():
    city = row['city']
    lat = row['lat']
    lon = row['lng']

    # print(f"\nFetching data for {city}...") # Removed this line

    pollution = get_air_quality(lat, lon)
    weather   = get_weather(lat, lon)
    osm_feat  = get_osm_features(city)

    if pollution and weather:
        # take first entry for pollution
        p = pollution['list'][0]
        poll_comp = p['components']
        poll_time = datetime.fromtimestamp(p['dt'], UTC)

        # take first entry for weather
        # The structure of current weather data is different from forecast data
        # Need to adjust how weather data is extracted
        w = weather # Current weather data is not a list
        main_weather = w['main']
        wind_weather = w['wind']
        clouds_weather = w['clouds']


        all_data.append({
            "city": city,
            "lat": lat,
            "lon": lon,
            "timestamp": poll_time,

            # Pollution
            "pm2_5": poll_comp['pm2_5'],
            "pm10": poll_comp['pm10'],
            "no2": poll_comp['no2'],
            "co": poll_comp['co'],
            "so2": poll_comp['so2'],
            "o3": poll_comp['o3'],

            # Weather - Adjusted for current weather data
            "temp_day": main_weather['temp'], # Using 'temp' for current temperature
            "temp_min": main_weather['temp_min'],
            "temp_max": main_weather['temp_max'],
            "humidity": main_weather['humidity'],
            "pressure": main_weather['pressure'],
            "wind_speed": wind_weather['speed'],
            "wind_deg": wind_weather.get('deg'), # 'deg' might not always be present
            "clouds": clouds_weather['all'],

            # OSM Features
            "road_count": osm_feat["road_count"],
            "industrial_count": osm_feat["industrial_count"],
            "dump_count": osm_feat["dump_count"],
            "farmland_count": osm_feat["farmland_count"]
        })

# -----------------------------
# Convert to DataFrame
# -----------------------------
final_df = pd.DataFrame(all_data)


# Save as CSV for later preprocessing
final_df.to_csv("city_air_weather_osm.csv", index=False)

print("Dataset is ready.")

import pandas as pd
import numpy as np

# Load CSV
df = pd.read_csv("/content/city_air_weather_osm.csv")

# -----------------------------
# 1. Handle Missing Values
# -----------------------------
numeric_cols = ['pm2_5', 'pm10', 'no2', 'co', 'so2', 'o3',
                'temp_day', 'temp_min', 'temp_max', 'humidity', 'pressure',
                'wind_speed', 'wind_deg', 'clouds',
                'road_count', 'industrial_count', 'dump_count', 'farmland_count']

for col in numeric_cols:
    df[col] = df[col].fillna(df[col].median())

# -----------------------------
# 2. Convert Timestamp
# -----------------------------
df['timestamp'] = pd.to_datetime(df['timestamp'])

# -----------------------------
# 3. Encode City Names
# -----------------------------
df['city_code'] = df['city'].astype('category').cat.codes

# -----------------------------
# 4. OSM Feature Engineering
# -----------------------------
# Example: convert raw counts to relative features
# You can normalize by max count across all cities
df['road_density'] = df['road_count'] / df['road_count'].max()
df['industrial_density'] = df['industrial_count'] / df['industrial_count'].max()
df['dump_density'] = df['dump_count'] / df['dump_count'].max()
df['farmland_density'] = df['farmland_count'] / df['farmland_count'].max()

# Optionally drop raw counts
df = df.drop(['road_count','industrial_count','dump_count','farmland_count'], axis=1)

# -----------------------------
# 5. Feature Scaling
# -----------------------------
from sklearn.preprocessing import MinMaxScaler

scaler_cols = ['pm2_5','pm10','no2','co','so2','o3',
               'temp_day','temp_min','temp_max','humidity','pressure',
               'wind_speed','wind_deg','clouds',
               'road_density','industrial_density','dump_density','farmland_density']

scaler = MinMaxScaler()
df[scaler_cols] = scaler.fit_transform(df[scaler_cols])

# -----------------------------
# 6. Save Preprocessed File
# -----------------------------
df.to_csv("/content/city_air_weather_osm_preprocessed.csv", index=False)

print("Preprocessing Done. Sample data:")
print(df.isnull().sum())

df=pd.read_csv("/content/city_air_weather_osm.csv")
# print(df.shape)
print(df.head())



import pandas as pd

# Load the preprocessed data (Step 1 of the plan)
try:
    df = pd.read_csv("/content/city_air_weather_osm_preprocessed.csv")
    print("Preprocessed data loaded successfully.")
    print(df.head())
except FileNotFoundError:
    print("Error: The file '/content/city_air_weather_osm_preprocessed.csv' was not found.")
    print("Please make sure the preprocessing step was completed successfully and the file exists.")
    df = None # Set df to None to avoid errors in subsequent steps

# Define and apply labeling rules (Steps 2 & 3 of the plan)
if df is not None:
    # Initialize a new column for pollution source labels
    df['pollution_source'] = 'Other' # Default label

    # Define thresholds for 'high' pollution levels (these are examples and may need adjustment)
    # These thresholds should ideally be based on domain knowledge or data distribution analysis
    threshold_no2 = df['no2'].quantile(0.75) # Example: 75th percentile
    threshold_so2 = df['so2'].quantile(0.75) # Example: 75th percentile
    threshold_pm = df['pm2_5'].quantile(0.75) # Using PM2.5 as a proxy for PM, adjust as needed

    # Apply labeling rules
    # Rule: Close to main road + high NO₂ = Vehicular
    df.loc[(df['road_density'] > df['road_density'].median()) & (df['no2'] > threshold_no2), 'pollution_source'] = 'Vehicular'

    # Rule: Near factory + high SO₂ = Industrial
    df.loc[(df['industrial_density'] > df['industrial_density'].median()) & (df['so2'] > threshold_so2), 'pollution_source'] = 'Industrial'

    # Rule: Near farmland + dry season + high PM = Agricultural
    # Assuming 'dry season' could be inferred from low humidity or high temperature (needs domain knowledge for India)
    # For simplicity here, we'll use a placeholder for 'dry season' or a combination of weather features
    # Let's use low humidity as a simple proxy for dry conditions for this example
    threshold_humidity = df['humidity'].quantile(0.25) # Example: 25th percentile (low humidity)

    df.loc[(df['farmland_density'] > df['farmland_density'].median()) & (df['humidity'] < threshold_humidity) & (df['pm2_5'] > threshold_pm), 'pollution_source'] = 'Agricultural'


    # Display the count of each label
    print("\nPollution Source Label Counts:")
    print(df['pollution_source'].value_counts())

    # Display head of the dataframe with new labels
    print("\nDataFrame with Pollution Source Labels:")
    print(df[['city', 'road_density', 'industrial_density', 'farmland_density', 'no2', 'so2', 'pm2_5', 'humidity', 'pollution_source']].head())

    # You might want to save this labeled dataset for the next steps
    df.to_csv("city_air_weather_osm_labeled.csv", index=False)

    print("\nLabeled dataset saved to 'city_air_weather_osm_labeled.csv'")



import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load the labeled dataset (Step 1 of the plan)
try:
    df = pd.read_csv("/content/city_air_weather_osm_labeled.csv")
    print("Labeled data loaded successfully.")
    # print(df.head())
except FileNotFoundError:
    print("Error: The file '/content/city_air_weather_osm_labeled.csv' was not found.")
    print("Please make sure the labeling step was completed successfully and the file exists.")
    df = None # Set df to None to avoid errors in subsequent steps

if df is not None:
    # 2. Prepare features and target variable (Step 2 of the plan)
    # Drop non-feature columns and the target variable for features
    features = df.drop(['city', 'lat', 'lon', 'timestamp', 'city_code', 'pollution_source'], axis=1)
    target = df['pollution_source']

    # Handle potential missing values if any remained after initial preprocessing
    features = features.fillna(features.median())


    # 3. Split data into training and testing sets (Step 3 of the plan)
    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42) # Removed stratify=target
    print(f"\nData split into training ({X_train.shape[0]} samples) and testing ({X_test.shape[0]} samples) sets.")


    # 4. Choose and initialize the Random Forest model (Step 4 of the plan)
    model = RandomForestClassifier(random_state=42)
    print("\nRandom Forest Classifier initialized.")

    # 5. Train the model (Step 5 of the plan)
    model.fit(X_train, y_train)
    print("Random Forest model trained.")

    # Make predictions on the test set
    y_pred = model.predict(X_test)

    # 6. Evaluate model performance (Step 7 of the plan)
    print("\nModel Evaluation:")
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix')
    plt.show()

!pip install xgboost

import pandas as pd
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Load the labeled dataset (Step 1 of the plan)
try:
    df = pd.read_csv("/content/city_air_weather_osm_labeled.csv")
    print("Labeled data loaded successfully.")
    # print(df.head())
except FileNotFoundError:
    print("Error: The file '/content/city_air_weather_osm_labeled.csv' was not found.")
    print("Please make sure the labeling step was completed successfully and the file exists.")
    df = None # Set df to None to avoid errors in subsequent steps

if df is not None:
    # Prepare features and target variable (Step 2 of the plan)
    # Drop non-feature columns and the target variable for features
    features = df.drop(['city', 'lat', 'lon', 'timestamp', 'city_code', 'pollution_source'], axis=1)
    target = df['pollution_source']

    # Handle potential missing values if any remained after initial preprocessing
    features = features.fillna(features.median())

    # Encode target variable using pd.factorize() (Alternative encoding)
    y_encoded, target_classes = pd.factorize(target)
    y_encoded = pd.Series(y_encoded) # Convert to Series to maintain index

    # Split data into training and testing sets (Step 3 of the plan)
    # Removed stratify due to small class sizes
    X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(features, y_encoded, test_size=0.2, random_state=42)
    print(f"\nData split into training ({X_train.shape[0]} samples) and testing ({X_test.shape[0]} samples) sets.")

    # Choose and initialize the XGBoost model (Step 4 of the plan)
    # Explicitly set objective and num_class for multi-class problems
    model = XGBClassifier(objective='multi:softprob', num_class=len(target_classes), random_state=42, use_label_encoder=False, eval_metric='mlogloss')
    print("\nXGBoost Classifier initialized.")

    # Train the model (Step 5 of the plan)
    model.fit(X_train, y_train_encoded)
    print("XGBoost model trained.")

    # Make predictions on the test set
    y_pred_encoded = model.predict(X_test)

    # Convert numerical predictions back to original labels for evaluation
    y_pred = pd.Series(y_pred_encoded).map(dict(enumerate(target_classes)))


    # Evaluate model performance (Step 7 of the plan)
    print("\nModel Evaluation:")
    print(f"Accuracy: {accuracy_score(y_test_encoded, y_pred_encoded):.4f}") # Evaluate with encoded labels

    print("\nClassification Report:")
    # Need to provide target_names to classification_report for original labels
    print(classification_report(y_test_encoded, y_pred_encoded, target_names=target_classes))

    # Confusion Matrix
    cm = confusion_matrix(y_test_encoded, y_pred_encoded) # Use encoded labels for confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_classes, yticklabels=target_classes) # Use original labels for plot
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix')
    plt.show()